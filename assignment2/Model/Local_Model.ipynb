{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reading and sampling the data\n",
    "\n",
    "def read_file(path):\n",
    "    \"\"\"\n",
    "    reads the file in pandas df and converts the date_time column to datetime type\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    return df\n",
    "\n",
    "def sample_on_srch_id(df, frac = 0.1):\n",
    "    \"\"\"\n",
    "    samples the dataframe based on the fraction of srach_id\n",
    "    \"\"\"\n",
    "    # get unique srch_ids\n",
    "    srch_ids = np.unique(df.srch_id)\n",
    "    # calculate how many ids to return\n",
    "    chosen_k = int(len(srch_ids) * frac)\n",
    "    # sample ids\n",
    "    chosen_ids = random.sample(list(srch_ids), k = chosen_k)\n",
    "    # filter the df to only have sampled ids\n",
    "    return df[df['srch_id'].isin(chosen_ids)]\n",
    "\n",
    "### Feature Engineering --------------------------\n",
    "\n",
    "## missing data ----------------------------------\n",
    "\n",
    "def remove_missing_values(df):\n",
    "    \"\"\"\n",
    "    removes columns with more than 50 percent missing data\n",
    "    \"\"\"\n",
    "    missing_values = df.isna().mean().round(4) * 100\n",
    "    missing_values = pd.DataFrame(missing_values).reset_index()\n",
    "    missing_values.columns = [\"column\", \"missing\"]\n",
    "    # filter where there are missing values\n",
    "    missing_values.query(\"missing > 50\", inplace=True)  # remove columns with more than 50 % of missing values\n",
    "    missing_values.sort_values(\"missing\", inplace=True)\n",
    "    #print(missing_values)\n",
    "    df.drop(missing_values.column, axis=1, inplace=True)\n",
    "\n",
    "def replace_missing_values(df):\n",
    "    \"\"\"\n",
    "    imputes missing values with -1\n",
    "    \"\"\"\n",
    "    df.fillna(value=-1, inplace=True) \n",
    "\n",
    "## new features ----------------------------------\n",
    "\n",
    "def extract_time(df):\n",
    "    \"\"\" \n",
    "    month, week, day of the week and hour of search\n",
    "    \"\"\"\n",
    "    df_datetime = pd.DatetimeIndex(df.date_time)\n",
    "    df[\"month\"] = df_datetime.month\n",
    "    df[\"week\"] = df_datetime.week\n",
    "    df[\"day\"] = df_datetime.dayofweek + 1\n",
    "    df[\"hour\"] = df_datetime.hour\n",
    "    del df['date_time']\n",
    "\n",
    "def new_historical_price(df):\n",
    "    \"\"\"\n",
    "    'unlogs' prop_log_historical_price column\n",
    "    \"\"\"\n",
    "    df[\"prop_historical_price\"] = (np.e ** df.prop_log_historical_price).replace(1.0, 0)\n",
    "    df.drop(\"prop_log_historical_price\", axis=1, inplace=True)\n",
    "\n",
    "def add_price_position(df, rank_type = \"dense\"):\n",
    "    \"\"\"\n",
    "    adds hotel price position (\"price_position\") inside \"srch_id\" column\n",
    "    \"\"\"\n",
    "    ranks = df.groupby('srch_id')['price_usd'].rank(ascending=True, method = rank_type)\n",
    "    df[\"price_position\"] = ranks\n",
    "\n",
    "\n",
    "def average_numerical_features(df, group_by = [\"prop_id\"], columns = [\"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_location_score2\"]):\n",
    "    \"\"\"\n",
    "    adds mean, median and standard deviation per prop_id (default) \n",
    "    for columns that are related to property (default)\n",
    "    \"\"\"\n",
    "    # caulcate means and rename columns\n",
    "    means = df.groupby(group_by)[columns].mean().reset_index()\n",
    "    means.columns = [means.columns[0]] + [x + \"_mean\" for x in means.columns[1:]]\n",
    "    # caulcate median and rename columns\n",
    "    medians = df.groupby(group_by)[columns].median().reset_index()\n",
    "    medians.columns = [medians.columns[0]] + [x + \"_median\" for x in medians.columns[1:]]\n",
    "    # caulcate means and rename columns\n",
    "    stds = df.groupby(group_by)[columns].std().reset_index()\n",
    "    stds.columns = [stds.columns[0]] + [x + \"_std\" for x in stds.columns[1:]]\n",
    "    ## attach aggregated data to the df\n",
    "    df = pd.merge(df, means, on=group_by)\n",
    "    df = pd.merge(df, medians, on=group_by)\n",
    "    df = pd.merge(df, stds, on=group_by)\n",
    "    return df\n",
    "\n",
    "def add_historical_booking_click(df):\n",
    "    \"\"\"\n",
    "    creates a column with the percentage of the prop_id booked/clicked rate overall\n",
    "    \"\"\"\n",
    "    # there are more prop_id in the test data than in train. \n",
    "    # Maybe we could still use this but would need to impute\n",
    "    # with the most common value (or something else)\n",
    "    \n",
    "    historical = df.groupby(\"prop_id\")[[\"click_bool\", \"booking_bool\"]].mean().reset_index()\n",
    "    historical.columns = [historical.columns[0]] + [x + \"_rate\" for x in historical.columns[1:]]\n",
    "    df = pd.merge(df, historical, on=\"prop_id\")\n",
    "    df.sort_values(\"srch_id\", inplace = True)\n",
    "    return df\n",
    "\n",
    "def join_historical_data(df, path = \"hist_click_book.csv\"):\n",
    "    \"\"\"\n",
    "    joins historical data according to prop_id. \n",
    "    path - location of historical data csv file\n",
    "    \n",
    "    \"\"\"\n",
    "    to_join = pd.read_csv(path)\n",
    "    joined = pd.merge(df, to_join, on=\"prop_id\")\n",
    "    return joined.sort_values(\"srch_id\")\n",
    "    \n",
    "    \n",
    "## other ----------------------------------\n",
    "\n",
    "def remove_cols(df, cols = [\"position\", \"prop_id\"]):\n",
    "    df.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "def remove_positions(df, positions = [5, 11, 17, 23]):\n",
    "    \"\"\"\n",
    "    removes hotels with specified positions \n",
    "    (based on the fact that hotels in those positions were not as booked)\n",
    "    \"\"\"\n",
    "    df = df[df[\"position\"].isin(positions) == False]\n",
    "\n",
    "def add_score(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    adds 'score' column to the df: 5 for booked, 1 for clicked\n",
    "    \"\"\"\n",
    "    \n",
    "    score = []\n",
    "    for book, click in zip(df.booking_bool, df.click_bool):\n",
    "        if book == 1:\n",
    "            score.append(5)\n",
    "            continue\n",
    "        if click == 1:\n",
    "            score.append(1)\n",
    "            continue\n",
    "        else:\n",
    "            score.append(0)\n",
    "    df[\"score\"] = score\n",
    "    del df['booking_bool']\n",
    "    del df['click_bool']\n",
    "\n",
    "def onehot(df, cols):\n",
    "    \"\"\" \n",
    "    returns a df with one-hot encoded columns (cols)\n",
    "    \"\"\"\n",
    "    \n",
    "    return pd.get_dummies(df, columns=cols)\n",
    "\n",
    "def sampling(df, target, method=\"undersampling\", frac=0.3):\n",
    "    \n",
    "    \"\"\"\n",
    "    df: input dataframe\n",
    "    targetcol: target column of majority class\n",
    "    method: specifies method of sampling - 'undersampling' or 'combination' of undersampling and oversampling.\n",
    "    frac: final fraction minority wrt majority class (default fraction 0.15/0.85)\n",
    "    \n",
    "    returns: df with undersampled majority and oversampled minority class\n",
    "    \n",
    "    note that this only has to be performed on the training data!\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Split df in minority and majority\n",
    "    minority = df.loc[df[target] > 0]\n",
    "    majority = df.loc[df[target] == 0]\n",
    "    lenmin = len(minority)\n",
    "    lenmaj = len(majority)\n",
    "    \n",
    "    # Calculate current fraction\n",
    "    frac_min = lenmin/(lenmin+lenmaj)\n",
    "    frac_maj = 1-frac_min\n",
    "    print(f\"Current fraction:\\nMinority class: {frac_min}, Majority class: {frac_maj}\")\n",
    "    \n",
    "    if method == \"undersampling\":\n",
    "        \n",
    "        sampling_frac = ((1-frac)/frac*lenmin)/lenmaj\n",
    "        sampled_df = df.groupby('srch_id').sample(frac=sampling_frac)\n",
    "        fin_frac = lenmin / (len(sampled_df) + lenmin)\n",
    "\n",
    "    elif method == \"combination\":\n",
    "        \n",
    "        # This still needs to be implemented\n",
    "        \n",
    "        return\n",
    "    else:\n",
    "        raise ExceptionError(\"Invalid argument for 'method'\")\n",
    "        \n",
    "    \n",
    "    print(f\"Final fraction:\\nMinority class: {fin_frac}, Majority class: {1-fin_frac}\")\n",
    "    \n",
    "    dfs = [minority, sampled_df]\n",
    "    finaldf = pd.concat(dfs)\n",
    "    finaldf.sort_values(\"srch_id\", inplace = True)\n",
    "    finaldf = finaldf.reset_index(drop=True)\n",
    "\n",
    "    print(\"Done\")\n",
    "    return finaldf\n",
    "\n",
    "\n",
    "### Feature engineering function -----------\n",
    "\n",
    "def feature_engineering_train(df):\n",
    "    \n",
    "    extract_time(df)\n",
    "    remove_missing_values(df)\n",
    "    replace_missing_values(df)\n",
    "    new_historical_price(df)\n",
    "    add_price_position(df)\n",
    "#     df = average_numerical_features(df)\n",
    "#     df = add_historical_booking_click(df)\n",
    "    add_score(df)\n",
    "    #remove_cols(df)\n",
    "    return df\n",
    "\n",
    "def feature_engineering_test(df):\n",
    "    \n",
    "    extract_time(df)\n",
    "    remove_missing_values(df)\n",
    "    replace_missing_values(df)\n",
    "    new_historical_price(df)\n",
    "    add_price_position(df)\n",
    "#     df = average_numerical_features(df)\n",
    "#     df = join_historical_data(df, path=\"data/hist_click_book.csv\")\n",
    "    return df\n",
    "    \n",
    "def create_df_queries_freq(df):\n",
    "    df_queries = pd.DataFrame()\n",
    "    df_queries = pd.crosstab(index=df['srch_id'], columns='count', colnames=['srch_id'])\n",
    "    df_queries.head()\n",
    "    df_queries.to_csv(\"../df_queries.csv\")\n",
    "    return pd.read_csv(\"../df_queries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/training_set_VU_DM.csv\")\n",
    "# testset = pd.read_csv(\"data/test_set_VU_DM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-987ae63e5a1e>:55: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  df[\"week\"] = df_datetime.week\n"
     ]
    }
   ],
   "source": [
    "# add historical bookings to test and trainingset\n",
    "\n",
    "df = feature_engineering_train(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# df = sampling(df, \"score\", method=\"undersampling\")\n",
    "# end = time.time()\n",
    "\n",
    "# print(f\"Execution took {end-start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df['prop_id']\n",
    "# del df['position']\n",
    "# del testset['prop_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainingset (df) should have 1 column more; the target column\n",
    "\n",
    "# len(testset.columns), len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>site_id</th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>prop_brand_bool</th>\n",
       "      <th>prop_location_score1</th>\n",
       "      <th>prop_location_score2</th>\n",
       "      <th>...</th>\n",
       "      <th>srch_saturday_night_bool</th>\n",
       "      <th>orig_destination_distance</th>\n",
       "      <th>random_bool</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>prop_historical_price</th>\n",
       "      <th>price_position</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>219</td>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0.0438</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>141.174964</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>219</td>\n",
       "      <td>10404</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>152.933013</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>219</td>\n",
       "      <td>21315</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>137.002613</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>219</td>\n",
       "      <td>27348</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>80.640419</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>219</td>\n",
       "      <td>29604</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>138.379512</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   srch_id  site_id  visitor_location_country_id  prop_country_id  prop_id  \\\n",
       "0        1       12                          187              219      893   \n",
       "1        1       12                          187              219    10404   \n",
       "2        1       12                          187              219    21315   \n",
       "3        1       12                          187              219    27348   \n",
       "4        1       12                          187              219    29604   \n",
       "\n",
       "   prop_starrating  prop_review_score  prop_brand_bool  prop_location_score1  \\\n",
       "0                3                3.5                1                  2.83   \n",
       "1                4                4.0                1                  2.20   \n",
       "2                3                4.5                1                  2.20   \n",
       "3                2                4.0                1                  2.83   \n",
       "4                4                3.5                1                  2.64   \n",
       "\n",
       "   prop_location_score2  ...  srch_saturday_night_bool  \\\n",
       "0                0.0438  ...                         1   \n",
       "1                0.0149  ...                         1   \n",
       "2                0.0245  ...                         1   \n",
       "3                0.0125  ...                         1   \n",
       "4                0.1241  ...                         1   \n",
       "\n",
       "   orig_destination_distance  random_bool  month  week  day  hour  \\\n",
       "0                       -1.0            1      4    14    4     8   \n",
       "1                       -1.0            1      4    14    4     8   \n",
       "2                       -1.0            1      4    14    4     8   \n",
       "3                       -1.0            1      4    14    4     8   \n",
       "4                       -1.0            1      4    14    4     8   \n",
       "\n",
       "   prop_historical_price  price_position  score  \n",
       "0             141.174964             3.0      0  \n",
       "1             152.933013            14.0      0  \n",
       "2             137.002613            15.0      0  \n",
       "3              80.640419            22.0      0  \n",
       "4             138.379512            11.0      0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import DMatrix\n",
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only useful for experimenting\n",
    "# Take a sample of 10 percent\n",
    "subsample = 0.3\n",
    "df = sample_on_srch_id(df, frac = subsample)\n",
    "del df['position']\n",
    "\n",
    "# split sample in 80 percent training and 20 percent test by srch_id\n",
    "gss = GroupShuffleSplit(test_size=.3, n_splits=1, random_state = 7).split(df, groups=df['srch_id'])\n",
    "\n",
    "X_train_inds, X_test_inds = next(gss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= df.iloc[X_train_inds]\n",
    "X_train = train_data.loc[:, ~train_data.columns.isin(['srch_id','score'])]\n",
    "y_train = train_data.loc[:, train_data.columns.isin(['score'])]\n",
    "del X_train['prop_id']\n",
    "\n",
    "groups = train_data.groupby('srch_id').size().to_frame('size')['size'].to_numpy()\n",
    "\n",
    "test_data= df.iloc[X_test_inds]\n",
    "\n",
    "#We need to keep the id for later predictions\n",
    "X_test = test_data.loc[:, ~test_data.columns.isin(['score'])]\n",
    "y_test = test_data.loc[:, test_data.columns.isin(['score'])]\n",
    "\n",
    "# save properties only of subset of data\n",
    "test_properties = X_test[\"prop_id\"].copy()\n",
    "del X_test['prop_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRanker(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "          colsample_bynode=1, colsample_bytree=0.9, eta=0.05, gamma=0,\n",
       "          gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
       "          learning_rate=0.05, max_delta_step=0, max_depth=10,\n",
       "          min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "          n_estimators=110, n_jobs=8, num_parallel_tree=1, random_state=42,\n",
       "          reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=0.75,\n",
       "          tree_method='hist', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBRanker(  \n",
    "    tree_method='hist',\n",
    "    booster='gbtree',\n",
    "    objective='rank:pairwise',\n",
    "    random_state=42, \n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.9, \n",
    "    eta=0.05, \n",
    "    max_depth=10, \n",
    "    n_estimators=110, \n",
    "    subsample=0.75,\n",
    "    )\n",
    "\n",
    "model.fit(X_train, y_train, group=groups, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, df):\n",
    "    return model.predict(df.loc[:, ~df.columns.isin(['srch_id'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not necessary right?\n",
    "\n",
    "del test_data['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (X_test.groupby('srch_id')\n",
    "               .apply(lambda x: predict(model, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "srch_id\n",
       "6         [-1.2162746, -0.6335337, -0.7467658, -0.018532...\n",
       "40        [0.34456602, 0.22548853, 0.3159296, 0.28564858...\n",
       "66        [-0.6149442, 0.45088935, -1.0186205, 1.1926656...\n",
       "87        [-0.018664833, 0.47724044, 0.08095994, -1.5373...\n",
       "103       [-0.2564619, -0.28451237, -0.6975368, -0.78128...\n",
       "                                ...                        \n",
       "332691    [0.81021637, 1.1025157, 0.91040015, -0.6342233...\n",
       "332710    [0.7214441, 1.0291793, 0.45252785, 0.20143427,...\n",
       "332729    [0.6239314, -0.47852698, -0.7261216, 1.3072062...\n",
       "332736    [-0.67050457, -1.0404496, -0.44885433, -1.0637...\n",
       "332755    [-1.2622985, -1.3274882, -1.3972553, -0.886477...\n",
       "Length: 17982, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG(predictions, df, path_idcg = \"idcg.csv\"):\n",
    "    \"\"\"\n",
    "    takes predicted positions and calulates average ndcg.\n",
    "    predictions - dataframe must have \"srch_id\" and \"prop_id\" ordered by relevance (inside \"srch_id\") (basically Lotte's model \"out\" dataframe)\n",
    "    df - test dataset (must contain \"srch_id\", \"prop_id\", \"score\")\n",
    "    path_idcg - path to idcg scores per \"srch_id\"\n",
    "    \"\"\"\n",
    "    # reset index \n",
    "    predictions.reset_index(drop = True, inplace = True)\n",
    "    # add position + 1\n",
    "    predictions[\"position\"] = predictions.groupby(by = ['srch_id']).cumcount()+2\n",
    "    # attach scores to predictions\n",
    "    predictions = pd.merge(predictions, df[[\"srch_id\", \"prop_id\", \"score\"]], on = [\"srch_id\", \"prop_id\"])\n",
    "    # calculate dcg\n",
    "    predictions[\"numerator\"] = np.power(2, predictions[\"score\"]) - 1\n",
    "    predictions[\"denominator\"] = np.log2(predictions[\"position\"])\n",
    "    predictions[\"intermediate_dcg\"] = predictions[\"numerator\"]/predictions[\"denominator\"]\n",
    "    dcg = predictions.groupby(\"srch_id\")[\"intermediate_dcg\"].sum().reset_index()\n",
    "    dcg.columns = [\"scrh_id\", \"DCG\"]\n",
    "    # read idcg\n",
    "    idcg = pd.read_csv(path_idcg)\n",
    "    # attach idcg to dcg\n",
    "    joined = pd.merge(dcg, idcg, on = \"scrh_id\")\n",
    "    # calculate NDCG\n",
    "    joined[\"NDCG\"] = joined[\"DCG\"]/joined[\"iDCG\"]\n",
    "    # calculate mean NDCG\n",
    "    return joined[\"NDCG\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reduced testdf for input in ndcg\n",
    "\n",
    "reduced_df = X_test.filter(['srch_id'], axis=1)\n",
    "reduced_df[\"prop_id\"] = test_properties\n",
    "reduced_df[\"score\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This could be done more efficiently\n",
    "\n",
    "# Prepare output file\n",
    "output = pd.DataFrame()\n",
    "output[\"srch_id\"] = test_data[\"srch_id\"].copy()\n",
    "output[\"prop_id\"] = test_properties.copy()\n",
    "\n",
    "# Add scores\n",
    "pred_scores_list = []\n",
    "\n",
    "for i in predictions:\n",
    "    for j in i:\n",
    "        pred_scores_list.append(j)      \n",
    "\n",
    "output[\"pred_scores\"] = pred_scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort on predicted_score output within srch_id\n",
    "\n",
    "out = output.groupby('srch_id').apply(pd.DataFrame.sort_values, 'pred_scores', ascending=False)\n",
    "del out[\"pred_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4976426371833703"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = NDCG(out, reduced_df, path_idcg = \"idcg.csv\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format output and Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Stop running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-505ddfdde409>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Stop running\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: Stop running"
     ]
    }
   ],
   "source": [
    "raise NotImplementedError(\"Stop running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This could be done more efficiently\n",
    "\n",
    "# Prepare output file\n",
    "output = pd.DataFrame()\n",
    "output[\"srch_id\"] = testset[\"srch_id\"]\n",
    "output[\"prop_id\"] = properties\n",
    "\n",
    "# Add scores\n",
    "pred_scores_list = []\n",
    "\n",
    "for i in predictions:\n",
    "    for j in i:\n",
    "        pred_scores_list.append(j)      \n",
    "\n",
    "output[\"pred_scores\"] = pred_scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort on predicted_score output within srch_id\n",
    "\n",
    "out = output.groupby('srch_id').apply(pd.DataFrame.sort_values, 'pred_scores', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del out[\"pred_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "\n",
    "out.to_csv('data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most important features\n",
    "\n",
    "xgb.plot_importance(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
