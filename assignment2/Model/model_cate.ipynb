{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acting-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-reach",
   "metadata": {},
   "source": [
    "### Feature eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "literary-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    \"\"\"\n",
    "    reads the file in pandas df and converts the date_time column to datetime type\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    return df\n",
    "\n",
    "def sample_on_srch_id(df, frac = 0.1):\n",
    "    \"\"\"\n",
    "    samples the dataframe based on the fraction of srach_id\n",
    "    \"\"\"\n",
    "    # get unique srch_ids\n",
    "    srch_ids = np.unique(df.srch_id)\n",
    "    # calculate how many ids to return\n",
    "    chosen_k = int(len(srch_ids) * frac)\n",
    "    # sample ids\n",
    "    chosen_ids = random.sample(list(srch_ids), k = chosen_k)\n",
    "    # filter the df to only have sampled ids\n",
    "    return df[df['srch_id'].isin(chosen_ids)]\n",
    "\n",
    "### Feature Engineering --------------------------\n",
    "\n",
    "## missing data ----------------------------------\n",
    "\n",
    "def remove_missing_values(df):\n",
    "    \"\"\"\n",
    "    removes columns with more than 50 percent missing data\n",
    "    \"\"\"\n",
    "    missing_values = df.isna().mean().round(4) * 100\n",
    "    missing_values = pd.DataFrame(missing_values).reset_index()\n",
    "    missing_values.columns = [\"column\", \"missing\"]\n",
    "    # filter where there are missing values\n",
    "    missing_values.query(\"missing > 50\", inplace=True)  # remove columns with more than 50 % of missing values\n",
    "    missing_values.sort_values(\"missing\", inplace=True)\n",
    "    #print(missing_values)\n",
    "    df.drop(missing_values.column, axis=1, inplace=True)\n",
    "\n",
    "def replace_missing_values(df):\n",
    "    \"\"\"\n",
    "    imputes missing values with -1\n",
    "    \"\"\"\n",
    "    df.fillna(value=-1, inplace=True) \n",
    "\n",
    "## new features ----------------------------------\n",
    "\n",
    "def extract_time(df):\n",
    "    \"\"\" \n",
    "    month, week, day of the week and hour of search\n",
    "    \"\"\"\n",
    "    df_datetime = pd.DatetimeIndex(df.date_time)\n",
    "    df[\"month\"] = df_datetime.month\n",
    "    df[\"week\"] = df_datetime.week\n",
    "    df[\"day\"] = df_datetime.dayofweek + 1\n",
    "    df[\"hour\"] = df_datetime.hour\n",
    "    del df['date_time']\n",
    "\n",
    "def new_historical_price(df):\n",
    "    \"\"\"\n",
    "    'unlogs' prop_log_historical_price column\n",
    "    \"\"\"\n",
    "    df[\"prop_historical_price\"] = (np.e ** df.prop_log_historical_price).replace(1.0, 0)\n",
    "    df.drop(\"prop_log_historical_price\", axis=1, inplace=True)\n",
    "\n",
    "def add_price_position(df, rank_type = \"dense\"):\n",
    "    \"\"\"\n",
    "    adds hotel price position (\"price_position\") inside \"srch_id\" column\n",
    "    \"\"\"\n",
    "    ranks = df.groupby('srch_id')['price_usd'].rank(ascending=True, method = rank_type)\n",
    "    df[\"price_position\"] = ranks\n",
    "\n",
    "\n",
    "def average_numerical_features(df, group_by = [\"prop_id\"], columns = [\"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_location_score2\"]):\n",
    "    \"\"\"\n",
    "    adds mean, median and standard deviation per prop_id (default) \n",
    "    for columns that are related to property (default)\n",
    "    \"\"\"\n",
    "    # caulcate means and rename columns\n",
    "    means = df.groupby(group_by)[columns].mean().reset_index()\n",
    "    means.columns = [means.columns[0]] + [x + \"_mean\" for x in means.columns[1:]]\n",
    "    # caulcate median and rename columns\n",
    "    medians = df.groupby(group_by)[columns].median().reset_index()\n",
    "    medians.columns = [medians.columns[0]] + [x + \"_median\" for x in medians.columns[1:]]\n",
    "    # caulcate means and rename columns\n",
    "    stds = df.groupby(group_by)[columns].std().reset_index()\n",
    "    stds.columns = [stds.columns[0]] + [x + \"_std\" for x in stds.columns[1:]]\n",
    "    ## attach aggregated data to the df\n",
    "    df = pd.merge(df, means, on=group_by)\n",
    "    df = pd.merge(df, medians, on=group_by)\n",
    "    df = pd.merge(df, stds, on=group_by)\n",
    "    return df\n",
    "\n",
    "def add_historical_booking_click(df):\n",
    "    \"\"\"\n",
    "    creates a column with the percentage of the prop_id booked/clicked rate overall\n",
    "    \"\"\"\n",
    "    # there are more prop_id in the test data than in train. \n",
    "    # Maybe we could still use this but would need to impute\n",
    "    # with the most common value (or something else)\n",
    "    \n",
    "    historical = df.groupby(\"prop_id\")[[\"click_bool\", \"booking_bool\"]].mean().reset_index()\n",
    "    historical.columns = [historical.columns[0]] + [x + \"_rate\" for x in historical.columns[1:]]\n",
    "    df = pd.merge(df, historical, on=\"prop_id\")\n",
    "    df.sort_values(\"srch_id\", inplace = True)\n",
    "    return df\n",
    "\n",
    "def join_historical_data(df, path = \"hist_click_book.csv\"):\n",
    "    \"\"\"\n",
    "    joins historical data according to prop_id. \n",
    "    path - location of historical data csv file\n",
    "    \n",
    "    \"\"\"\n",
    "    to_join = pd.read_csv(path)\n",
    "    joined = pd.merge(df, to_join, on=\"prop_id\")\n",
    "    return joined.sort_values(\"srch_id\")\n",
    "    \n",
    "    \n",
    "## other ----------------------------------\n",
    "\n",
    "def remove_cols(df, cols = [\"position\"]):\n",
    "    df.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "def remove_positions(df, positions = [5, 11, 17, 23]):\n",
    "    \"\"\"\n",
    "    removes hotels with specified positions \n",
    "    (based on the fact that hotels in those positions were not as booked)\n",
    "    \"\"\"\n",
    "    df = df[df[\"position\"].isin(positions) == False]\n",
    "\n",
    "def add_score(df):\n",
    "    \"\"\"\n",
    "    adds 'score' column to the df: 5 for booked, 1 for clicked\n",
    "    \"\"\"\n",
    "    score = []\n",
    "    for book, click in zip(df.booking_bool, df.click_bool):\n",
    "        if book == 1:\n",
    "            score.append(5)\n",
    "            continue\n",
    "        if click == 1:\n",
    "            score.append(1)\n",
    "            continue\n",
    "        else:\n",
    "            score.append(0)\n",
    "    df[\"score\"] = score\n",
    "    del df['booking_bool']\n",
    "    del df['click_bool']\n",
    "\n",
    "def onehot(df, cols):\n",
    "    \"\"\" \n",
    "    returns a df with one-hot encoded columns (cols)\n",
    "    \"\"\"\n",
    "    \n",
    "    return pd.get_dummies(df, columns=cols)\n",
    "\n",
    "\n",
    "### Feature engineering function -----------\n",
    "\n",
    "def feature_engineering_train(df):\n",
    "    \n",
    "    extract_time(df)\n",
    "    remove_missing_values(df)\n",
    "    replace_missing_values(df)\n",
    "    new_historical_price(df)\n",
    "    add_price_position(df)\n",
    "    #df = average_numerical_features(df)\n",
    "    #df = add_historical_booking_click(df)\n",
    "    add_score(df)\n",
    "    remove_cols(df)\n",
    "    return df\n",
    "\n",
    "def feature_engineering_test(df):\n",
    "    \n",
    "    extract_time(df)\n",
    "    remove_missing_values(df)\n",
    "    replace_missing_values(df)\n",
    "    new_historical_price(df)\n",
    "    add_price_position(df)\n",
    "    #df = average_numerical_features(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "commercial-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG(predictions, df, path_idcg = \"idcg.csv\"):\n",
    "    \"\"\"\n",
    "    takes predicted positions and calulates average ndcg.\n",
    "    predictions - dataframe must have \"srch_id\" and \"prop_id\" ordered by relevance (inside \"srch_id\") (basically Lotte's model \"out\" dataframe)\n",
    "    df - training dataset (must contain \"srch_id\", \"prop_id\", \"score\")\n",
    "    path_idcg - path to idcg scores per \"srch_id\"\n",
    "    \"\"\"\n",
    "    # reset index \n",
    "    predictions.reset_index(drop = True, inplace = True)\n",
    "    # add position + 1\n",
    "    predictions[\"position\"] = predictions.groupby(by = ['srch_id']).cumcount()+1\n",
    "    # filter to only have positions up to 5\n",
    "    predictions = predictions[predictions.position < 6]\n",
    "    # attach scores to predictions\n",
    "    predictions = pd.merge(predictions, df[[\"srch_id\", \"prop_id\", \"score\"]], on = [\"srch_id\", \"prop_id\"])\n",
    "    predictions[\"numerator\"] = predictions[\"score\"]\n",
    "    predictions[\"denominator\"] = np.log2(predictions[\"position\"])\n",
    "    predictions.loc[predictions.position == 1, \"denominator\"] = 1\n",
    "    predictions[\"intermediate_dcg\"] = predictions[\"numerator\"]/predictions[\"denominator\"]\n",
    "    dcg = predictions.groupby(\"srch_id\")[\"intermediate_dcg\"].sum().reset_index()\n",
    "    dcg.columns = [\"scrh_id\", \"DCG\"]\n",
    "    # read idcg\n",
    "    idcg = pd.read_csv(path_idcg)\n",
    "    # attach idcg to dcg\n",
    "    joined = pd.merge(dcg, idcg, on = \"scrh_id\")\n",
    "    # calculate NDCG\n",
    "    joined[\"NDCG\"] = joined[\"DCG\"]/joined[\"iDCG\"]\n",
    "    # calculate mean NDCG\n",
    "    return joined[\"NDCG\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quantitative-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, df):\n",
    "    return model.predict(df.loc[:, ~df.columns.isin(['srch_id'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "checked-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(df_mod, normalizing_var, column):\n",
    "    # df_mod = dataframe\n",
    "    # normalizing_var = variable that will be used for normalizing\n",
    "    # column = variable that will be normalized\n",
    "\n",
    "    methods = [\"mean\", \"std\"]\n",
    "\n",
    "    df = df_mod.groupby(normalizing_var).agg({column: methods})\n",
    "\n",
    "    df.columns = df.columns.droplevel()\n",
    "    col = {}\n",
    "    for method in methods:\n",
    "        col[method] = column + \"_\" + method\n",
    "\n",
    "    df.rename(columns=col, inplace=True)\n",
    "    df_merge = df_mod.merge(df.reset_index(), on=normalizing_var)\n",
    "    df_merge[column + \"_norm_by_\" + normalizing_var] = (\n",
    "        df_merge[column] - df_merge[column + \"_mean\"]\n",
    "    ) / df_merge[column + \"_std\"]\n",
    "    df_merge = df_merge.drop(labels=[col[\"mean\"], col[\"std\"]], axis=1)\n",
    "\n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-hybrid",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lined-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import DMatrix\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "featured-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../data/test_set_VU_DM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mathematical-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_file(\"../data/training_set_VU_DM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "prescription-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_test = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "allied-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_train = df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "sonic-height",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-679a72da8fd3>:53: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  df[\"week\"] = df_datetime.week\n",
      "<ipython-input-107-679a72da8fd3>:53: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  df[\"week\"] = df_datetime.week\n"
     ]
    }
   ],
   "source": [
    "df = feature_engineering_train(d1_train)\n",
    "testset = feature_engineering_test(d1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "little-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "#properties = testset['prop_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "swiss-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df['prop_id']\n",
    "#del df['position']\n",
    "#del testset['prop_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "impaired-program",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4958347 entries, 0 to 4958346\n",
      "Data columns (total 28 columns):\n",
      " #   Column                       Dtype  \n",
      "---  ------                       -----  \n",
      " 0   srch_id                      int64  \n",
      " 1   site_id                      int64  \n",
      " 2   visitor_location_country_id  int64  \n",
      " 3   prop_country_id              int64  \n",
      " 4   prop_id                      int64  \n",
      " 5   prop_starrating              int64  \n",
      " 6   prop_review_score            float64\n",
      " 7   prop_brand_bool              int64  \n",
      " 8   prop_location_score1         float64\n",
      " 9   prop_location_score2         float64\n",
      " 10  price_usd                    float64\n",
      " 11  promotion_flag               int64  \n",
      " 12  srch_destination_id          int64  \n",
      " 13  srch_length_of_stay          int64  \n",
      " 14  srch_booking_window          int64  \n",
      " 15  srch_adults_count            int64  \n",
      " 16  srch_children_count          int64  \n",
      " 17  srch_room_count              int64  \n",
      " 18  srch_saturday_night_bool     int64  \n",
      " 19  orig_destination_distance    float64\n",
      " 20  random_bool                  int64  \n",
      " 21  month                        int64  \n",
      " 22  week                         int64  \n",
      " 23  day                          int64  \n",
      " 24  hour                         int64  \n",
      " 25  prop_historical_price        float64\n",
      " 26  price_position               float64\n",
      " 27  score                        int64  \n",
      "dtypes: float64(7), int64(21)\n",
      "memory usage: 1.0 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-colors",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dying-sleep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4958347 entries, 0 to 4958346\n",
      "Data columns (total 42 columns):\n",
      " #   Column                                     Dtype  \n",
      "---  ------                                     -----  \n",
      " 0   srch_id                                    int64  \n",
      " 1   site_id                                    int64  \n",
      " 2   visitor_location_country_id                int64  \n",
      " 3   prop_country_id                            int64  \n",
      " 4   prop_id                                    int64  \n",
      " 5   prop_starrating                            int64  \n",
      " 6   prop_review_score                          float64\n",
      " 7   prop_brand_bool                            int64  \n",
      " 8   prop_location_score1                       float64\n",
      " 9   prop_location_score2                       float64\n",
      " 10  price_usd                                  float64\n",
      " 11  promotion_flag                             int64  \n",
      " 12  srch_destination_id                        int64  \n",
      " 13  srch_length_of_stay                        int64  \n",
      " 14  srch_booking_window                        int64  \n",
      " 15  srch_adults_count                          int64  \n",
      " 16  srch_children_count                        int64  \n",
      " 17  srch_room_count                            int64  \n",
      " 18  srch_saturday_night_bool                   int64  \n",
      " 19  orig_destination_distance                  float64\n",
      " 20  random_bool                                int64  \n",
      " 21  month                                      int64  \n",
      " 22  week                                       int64  \n",
      " 23  day                                        int64  \n",
      " 24  hour                                       int64  \n",
      " 25  prop_historical_price                      float64\n",
      " 26  price_position                             float64\n",
      " 27  score                                      int64  \n",
      " 28  prop_starrating_norm_by_srch_id            float64\n",
      " 29  prop_review_score_norm_by_srch_id          float64\n",
      " 30  prop_location_score1_norm_by_srch_id       float64\n",
      " 31  prop_location_score2_norm_by_srch_id       float64\n",
      " 32  price_usd_norm_by_srch_id                  float64\n",
      " 33  srch_adults_count_norm_by_srch_id          float64\n",
      " 34  srch_children_count_norm_by_srch_id        float64\n",
      " 35  srch_room_count_norm_by_srch_id            float64\n",
      " 36  orig_destination_distance_norm_by_srch_id  float64\n",
      " 37  month_norm_by_srch_id                      float64\n",
      " 38  week_norm_by_srch_id                       float64\n",
      " 39  day_norm_by_srch_id                        float64\n",
      " 40  prop_historical_price_norm_by_srch_id      float64\n",
      " 41  price_position_norm_by_srch_id             float64\n",
      "dtypes: float64(21), int64(21)\n",
      "memory usage: 1.6 GB\n"
     ]
    }
   ],
   "source": [
    "target_list = [\"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_location_score2\",\n",
    "              \"price_usd\", \"srch_adults_count\", \"srch_children_count\", \"srch_room_count\", \"orig_destination_distance\",\n",
    "              \"month\", \"week\", \"day\", \"prop_historical_price\", \"price_position\"]\n",
    "for column in target_list:\n",
    "    df = normalize_features(df, group_key=\"srch_id\", target_column=column)\n",
    "    testset = normalize_features(testset, group_key=\"srch_id\", target_column=column)\n",
    "testset.info()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "immediate-trading",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-westminster",
   "metadata": {},
   "source": [
    "### Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "domestic-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_model(df, learning_rate, colsample_bytree, max_depth, n_estimators):\n",
    "    \n",
    "    # data\n",
    "    gss = GroupShuffleSplit(test_size=.3, n_splits=1, random_state = 7).split(df, groups=df['srch_id'])\n",
    "\n",
    "    X_train_inds, X_test_inds = next(gss)\n",
    "    train_data= df.iloc[X_train_inds]\n",
    "    test_data= df.iloc[X_test_inds]\n",
    "    properties = test_data['prop_id']\n",
    "    del train_data['prop_id']\n",
    "    del test_data['prop_id']\n",
    "\n",
    "\n",
    "    X_train = train_data.loc[:, ~train_data.columns.isin(['srch_id','score'])]\n",
    "\n",
    "    y_train = train_data.loc[:, train_data.columns.isin(['score'])]\n",
    "\n",
    "    groups = train_data.groupby('srch_id').size().to_frame('size')['size'].to_numpy()\n",
    "\n",
    "\n",
    "    #We need to keep the id for later predictions\n",
    "    X_test = test_data.loc[:, ~test_data.columns.isin(['score'])]\n",
    "    y_test = test_data.loc[:, test_data.columns.isin(['score'])]\n",
    "\n",
    "\n",
    "    model = xgb.XGBRanker(  \n",
    "    tree_method='hist',\n",
    "    booster='gbtree',\n",
    "    objective='rank:pairwise',\n",
    "    #eval_metric = [\"ndcg\", \"map\"],\n",
    "    random_state=42,    \n",
    "    learning_rate=learning_rate,\n",
    "    colsample_bytree=colsample_bytree, \n",
    "    #eta=eta, \n",
    "    max_depth=max_depth, \n",
    "    n_estimators=n_estimators, \n",
    "    subsample=subsample \n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, group=groups, verbose=True)\n",
    "    \n",
    "\n",
    "    predictions = (X_test.groupby('srch_id').apply(lambda x: predict(model, x)))\n",
    "    output = pd.DataFrame()\n",
    "    output[\"srch_id\"] = test_data[\"srch_id\"]\n",
    "    output[\"prop_id\"] = properties\n",
    "\n",
    "    # Add scores\n",
    "    pred_scores_list = []\n",
    "\n",
    "    for i in predictions:\n",
    "        for j in i:\n",
    "            pred_scores_list.append(j)      \n",
    "\n",
    "    output[\"pred_scores\"] = pred_scores_list\n",
    "    \n",
    "    out = output.groupby('srch_id').apply(pd.DataFrame.sort_values, 'pred_scores', ascending=False)\n",
    "    del out[\"pred_scores\"]\n",
    "    #out.to_csv('../data/submission_cate.csv', index=False)\n",
    "    \n",
    "    return NDCG(out, df, path_idcg = \"idcg.csv\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "exempt-arrest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5252502184628943"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduct = sample_on_srch_id(df, frac = 0.1)\n",
    "\n",
    "learning_rate=0.1\n",
    "colsample_bytree=0.9\n",
    "max_depth=6\n",
    "n_estimators=110\n",
    "subsample=0.75\n",
    "tuning_model(df, learning_rate, colsample_bytree, max_depth, n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "small-mountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 6 110 2\n",
      "0.05 6 150 3\n",
      "0.05 6 200 4\n",
      "0.05 8 110 5\n",
      "0.05 8 150 6\n",
      "0.05 8 200 7\n",
      "0.05 10 110 8\n",
      "0.05 10 150 9\n",
      "0.05 10 200 10\n",
      "0.1 6 110 11\n",
      "0.1 6 150 12\n",
      "0.1 6 200 13\n",
      "0.1 8 110 14\n",
      "0.1 8 150 15\n",
      "0.1 8 200 16\n",
      "0.1 10 110 17\n",
      "0.1 10 150 18\n",
      "0.1 10 200 19\n",
      "0.3 6 110 20\n",
      "0.3 6 150 21\n",
      "0.3 6 200 22\n",
      "0.3 8 110 23\n",
      "0.3 8 150 24\n",
      "0.3 8 200 25\n",
      "0.3 10 110 26\n",
      "0.3 10 150 27\n",
      "0.3 10 200 28\n"
     ]
    }
   ],
   "source": [
    "eta_ = [0.05, 0.1, 0.3]\n",
    "max_depth_=[6,8,10] # Maximum depth of a tree\n",
    "n_estimators_ =[110, 150, 200]\n",
    "\n",
    "e = []\n",
    "d = []\n",
    "est = []\n",
    "ndcg = []\n",
    "i = 1\n",
    "\n",
    "df_reduct = sample_on_srch_id(df, frac = 0.1)\n",
    "\n",
    "\n",
    "for eta in eta_:\n",
    "    for max_depth in max_depth_:\n",
    "        for n_estimators in n_estimators_:\n",
    "            i += 1\n",
    "            ndcg.append(tuning_model(df_reduct, learning_rate, colsample_bytree, eta, max_depth, n_estimators))\n",
    "            e.append(eta)\n",
    "            d.append(max_depth)\n",
    "            est.append(n_estimators)\n",
    "            print(eta, max_depth, n_estimators, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "magnetic-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eta  max_depth  n_estimators      NDCG\n",
      "0   0.05          6           110  0.434470\n",
      "1   0.05          6           150  0.437175\n",
      "2   0.05          6           200  0.436196\n",
      "3   0.05          8           110  0.428122\n",
      "4   0.05          8           150  0.429067\n",
      "5   0.05          8           200  0.428209\n",
      "6   0.05         10           110  0.418455\n",
      "7   0.05         10           150  0.415956\n",
      "8   0.05         10           200  0.408828\n",
      "9   0.10          6           110  0.434470\n",
      "10  0.10          6           150  0.437175\n",
      "11  0.10          6           200  0.436196\n",
      "12  0.10          8           110  0.428122\n",
      "13  0.10          8           150  0.429067\n",
      "14  0.10          8           200  0.428209\n",
      "15  0.10         10           110  0.418455\n",
      "16  0.10         10           150  0.415956\n",
      "17  0.10         10           200  0.408828\n",
      "18  0.30          6           110  0.434470\n",
      "19  0.30          6           150  0.437175\n",
      "20  0.30          6           200  0.436196\n",
      "21  0.30          8           110  0.428122\n",
      "22  0.30          8           150  0.429067\n",
      "23  0.30          8           200  0.428209\n",
      "24  0.30         10           110  0.418455\n",
      "25  0.30         10           150  0.415956\n",
      "26  0.30         10           200  0.408828\n"
     ]
    }
   ],
   "source": [
    "tuning = pd.DataFrame(list(zip(e, d, est, ndcg)),\n",
    "               columns =['eta', 'max_depth', 'n_estimators', 'NDCG'])\n",
    "print(tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "statutory-upgrade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>objective</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.050</td>\n",
       "      <td>6</td>\n",
       "      <td>500</td>\n",
       "      <td>rank:pairwise</td>\n",
       "      <td>0.435339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.050</td>\n",
       "      <td>6</td>\n",
       "      <td>300</td>\n",
       "      <td>rank:pairwise</td>\n",
       "      <td>0.433845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.050</td>\n",
       "      <td>6</td>\n",
       "      <td>500</td>\n",
       "      <td>rank:ndcg</td>\n",
       "      <td>0.433500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>6</td>\n",
       "      <td>500</td>\n",
       "      <td>rank:pairwise</td>\n",
       "      <td>0.431737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>6</td>\n",
       "      <td>300</td>\n",
       "      <td>rank:pairwise</td>\n",
       "      <td>0.431419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>0</td>\n",
       "      <td>0.400</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>rank:map</td>\n",
       "      <td>0.359689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>0</td>\n",
       "      <td>0.400</td>\n",
       "      <td>15</td>\n",
       "      <td>110</td>\n",
       "      <td>rank:pairwise</td>\n",
       "      <td>0.358660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>0</td>\n",
       "      <td>0.400</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>rank:pairwise</td>\n",
       "      <td>0.358505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>0</td>\n",
       "      <td>0.400</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>rank:map</td>\n",
       "      <td>0.358086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>0</td>\n",
       "      <td>0.400</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>rank:pairwise</td>\n",
       "      <td>0.355074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  learning_rate  max_depth  n_estimators      objective  \\\n",
       "0             0          0.050          6           500  rank:pairwise   \n",
       "1             0          0.050          6           300  rank:pairwise   \n",
       "2             0          0.050          6           500      rank:ndcg   \n",
       "3             0          0.025          6           500  rank:pairwise   \n",
       "4             0          0.100          6           300  rank:pairwise   \n",
       "..          ...            ...        ...           ...            ...   \n",
       "625           0          0.400         12           150       rank:map   \n",
       "626           0          0.400         15           110  rank:pairwise   \n",
       "627           0          0.400         12           150  rank:pairwise   \n",
       "628           0          0.400         12           110       rank:map   \n",
       "629           0          0.400         12           110  rank:pairwise   \n",
       "\n",
       "         NDCG  \n",
       "0    0.435339  \n",
       "1    0.433845  \n",
       "2    0.433500  \n",
       "3    0.431737  \n",
       "4    0.431419  \n",
       "..        ...  \n",
       "625  0.359689  \n",
       "626  0.358660  \n",
       "627  0.358505  \n",
       "628  0.358086  \n",
       "629  0.355074  \n",
       "\n",
       "[630 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning = pd.read_csv(\"tuning/tuning_results.csv\")\n",
    "tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-chess",
   "metadata": {},
   "source": [
    "### Tuning normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dangerous-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sample_on_srch_id(df, frac = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "silver-findings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prop_starrating\n",
      "prop_location_score2\n",
      "prop_historical_price\n",
      "price_usd\n",
      "prop_review_score\n",
      "price_position\n",
      "prop_location_score1\n",
      "orig_destination_distance\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.35\n",
    "colsample_bytree=0.9\n",
    "max_depth=6\n",
    "n_estimators=300\n",
    "\n",
    "ndcg = []\n",
    "col = []\n",
    "\n",
    "target_list = [\"prop_starrating\", \"prop_location_score2\", \"prop_historical_price\", \"price_usd\",\n",
    "               \"prop_review_score\",  \"price_position\", \"prop_location_score1\", \"orig_destination_distance\"]\n",
    "\n",
    "for column in target_list:\n",
    "    testset_1 = normalize_features(testset, group_key=\"srch_id\", target_column=column)\n",
    "    df_1 = normalize_features(df, group_key=\"srch_id\", target_column=column)\n",
    "    ndcg.append(tuning_model(df, learning_rate, colsample_bytree, max_depth, n_estimators))\n",
    "    col.append(column)\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "prime-circulation",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-3a9a9a38bbd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdf_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_reduct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prop_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mndcg_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuning_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolsample_bytree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mndcg_not_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuning_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_reduct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolsample_bytree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-9dcca2b5c343>\u001b[0m in \u001b[0;36mtuning_model\u001b[0;34m(df, learning_rate, colsample_bytree, max_depth, n_estimators)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred_scores\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_scores_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'srch_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pred_scores'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred_scores\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m#out.to_csv('../data/submission_cate.csv', index=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode.chained_assignment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m                 \u001b[0;31m# gh-20949\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f, data)\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \"\"\"\n\u001b[0;32m--> 928\u001b[0;31m         \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    202\u001b[0m         ):\n\u001b[1;32m    203\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mresult_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mlibreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidApply\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mfast_apply\u001b[0;34m(self, f, sdata, names)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# must return keys::list, values::list, mutated::bool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mstarts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlibreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_frame_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_chop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.apply_frame_axis0\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m   5990\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5991\u001b[0m         \"\"\"\n\u001b[0;32m-> 5992\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5993\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5994\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"copy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"copy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ndcg_norm = []\n",
    "ndcg_not_norm = []\n",
    "s = []\n",
    "\n",
    "target_list = [\"prop_starrating\", \"prop_location_score2\", \"prop_historical_price\", \"price_usd\",\n",
    "               \"prop_review_score\",  \"price_position\", \"prop_location_score1\", \"orig_destination_distance\"]\n",
    "\n",
    "for sim in range(5):\n",
    "    df_reduct = sample_on_srch_id(df, frac = 0.1)\n",
    "    for column in target_list:\n",
    "        df_norm = normalize_features(df_reduct, \"prop_id\", column)\n",
    "    ndcg_norm.append(tuning_model(df_norm, learning_rate, colsample_bytree, max_depth, n_estimators))\n",
    "    ndcg_not_norm.append(tuning_model(df_reduct, learning_rate, colsample_bytree, max_depth, n_estimators))\n",
    "    s.append(sim)\n",
    "    print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "alone-notification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "      <th>ndcg_norm</th>\n",
       "      <th>ndcg_not_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.412109</td>\n",
       "      <td>0.412109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.581139</td>\n",
       "      <td>0.004198</td>\n",
       "      <td>0.004198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407040</td>\n",
       "      <td>0.407040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.409484</td>\n",
       "      <td>0.409484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.411246</td>\n",
       "      <td>0.411246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.415872</td>\n",
       "      <td>0.415872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.416905</td>\n",
       "      <td>0.416905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sim  ndcg_norm  ndcg_not_norm\n",
       "count  5.000000   5.000000       5.000000\n",
       "mean   2.000000   0.412109       0.412109\n",
       "std    1.581139   0.004198       0.004198\n",
       "min    0.000000   0.407040       0.407040\n",
       "25%    1.000000   0.409484       0.409484\n",
       "50%    2.000000   0.411246       0.411246\n",
       "75%    3.000000   0.415872       0.415872\n",
       "max    4.000000   0.416905       0.416905"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_feat = pd.DataFrame(list(zip(ndcg_norm, ndcg_not_norm)), columns=[\"ndcg_norm\", \"ndcg_not_norm\"])\n",
    "ndcg_feat.to_csv(\"tuning/normalization_prop_id_8.csv\")\n",
    "ndcg_feat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-ballot",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "useful-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list_s = [\"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_location_score2\"]\n",
    "for column in target_list_s:\n",
    "    df = normalize_features(df, \"srch_id\", column)\n",
    "    testset = normalize_features(testset, \"srch_id\", column)\n",
    "\n",
    "\n",
    "target_list_p = [\"prop_starrating\", \"prop_location_score2\", \"prop_historical_price\", \"price_usd\",\n",
    "               \"prop_review_score\",  \"price_position\", \"prop_location_score1\", \"orig_destination_distance\"]\n",
    "for column in target_list_p:\n",
    "    df = normalize_features(df, \"srch_id\", column)\n",
    "    testset = normalize_features(testset, \"srch_id\", column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "informed-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save properties\n",
    "properties = testset['prop_id']\n",
    "\n",
    "# delete\n",
    "del df['prop_id']\n",
    "del testset['prop_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "southern-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:, ~df.columns.isin(['srch_id','score'])]\n",
    "y_train = df.loc[:, df.columns.isin(['score'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "pending-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby('srch_id').size().to_frame('size')['size'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "union-tennessee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRanker(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "          colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=-1,\n",
       "          importance_type='gain', interaction_constraints='',\n",
       "          learning_rate=0.35, max_delta_step=0, max_depth=6, min_child_weight=1,\n",
       "          missing=nan, monotone_constraints='()', n_estimators=300, n_jobs=8,\n",
       "          num_parallel_tree=1, objective='rank:ndcg', random_state=42,\n",
       "          reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=0.75,\n",
       "          tree_method='hist', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBRanker(  \n",
    "    tree_method='hist',\n",
    "    booster='gbtree',\n",
    "    objective='rank:ndcg',\n",
    "    random_state=42, \n",
    "    learning_rate=0.35,\n",
    "    colsample_bytree=0.9, \n",
    "    max_depth=6, \n",
    "    n_estimators=300, \n",
    "    subsample=0.75,\n",
    "    )\n",
    "\n",
    "model.fit(X_train, y_train, group=groups, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (testset.groupby('srch_id')\n",
    "               .apply(lambda x: predict(model, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "prescribed-terrain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "srch_id\n",
       "1         [0.118569724, 0.14353089, -0.090653494, -0.158...\n",
       "3         [-1.4137175, -0.88320905, -2.0333762, -1.24059...\n",
       "6         [0.18259262, -0.41942, 0.7821759, 0.50223756, ...\n",
       "7         [-1.9045314, -1.0106854, -0.47945878, -0.98062...\n",
       "10        [-0.9869655, -1.235008, -0.83471453, -1.225295...\n",
       "                                ...                        \n",
       "332781    [-0.13198242, -0.764231, -0.00097529846, -0.86...\n",
       "332783    [-1.497115, 0.8490884, 0.42351252, -0.4794531,...\n",
       "332785    [1.1425006, -0.60748386, 1.9364889, 0.8171756,...\n",
       "332786    [0.15598962, 0.2120623, -0.09032531, 1.5351955...\n",
       "332787    [2.1848752, 1.3507464, 0.9365438, 1.8715631, -...\n",
       "Length: 199549, dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model)\n",
    "#target_list = [\"prop_starrating\", \"prop_review_score\", \"prop_location_score1\", \"prop_location_score2\",\n",
    "#              \"price_usd\", \"srch_adults_count\", \"srch_children_count\", \"srch_room_count\", \"orig_destination_distance\",\n",
    "#              \"month\", \"week\", \"day\", \"prop_historical_price\", \"price_position\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "allied-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This could be done more efficiently\n",
    "\n",
    "# Prepare output file\n",
    "output = pd.DataFrame()\n",
    "output[\"srch_id\"] = testset[\"srch_id\"]\n",
    "output[\"prop_id\"] = properties\n",
    "\n",
    "# Add scores\n",
    "pred_scores_list = []\n",
    "\n",
    "for i in predictions:\n",
    "    for j in i:\n",
    "        pred_scores_list.append(j)      \n",
    "\n",
    "output[\"pred_scores\"] = pred_scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "pacific-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = output.groupby('srch_id').apply(pd.DataFrame.sort_values, 'pred_scores', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "smooth-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "del out[\"pred_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "following-highland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>prop_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srch_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>99484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>54937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>61934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>28181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            srch_id  prop_id\n",
       "srch_id                     \n",
       "1       23        1    99484\n",
       "        9         1    54937\n",
       "        12        1    61934\n",
       "        5         1    28181\n",
       "        4         1    24194"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "rough-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv('../data/submission_normalization4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-stock",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
